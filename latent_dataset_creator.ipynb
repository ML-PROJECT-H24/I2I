{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "import torchvision\n",
    "import tqdm\n",
    "\n",
    "from diffusers.models import AutoencoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = 'data/faces4/256x256/faces'\n",
    "LATENT_IMAGE_FOLDER = 'data/latent_faces4'\n",
    "TARGET_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def init_vae(device):\n",
    "    # https://huggingface.co/stabilityai/sd-vae-ft-mse\n",
    "    model: AutoencoderKL = AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-mse').to(device)\n",
    "    torch.compile\n",
    "    model = model.eval()\n",
    "    model.train = False\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "vae = init_vae(device)\n",
    "scale_factor=0.18215 # scale_factor follows DiT and stable diffusion.\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]: \n",
    "    posterior = vae.encode(x, return_dict=False)[0].parameters\n",
    "    return torch.chunk(posterior, 2, dim=1)    \n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(mean: torch.FloatTensor, logvar: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    z = torch.randn_like(mean)\n",
    "    z = mean + z * std\n",
    "    return z * scale_factor\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode(z) -> torch.Tensor:\n",
    "    x = vae.decode(z / scale_factor, return_dict=False)[0]\n",
    "    x = ((x + 1.0) * 127.5).clamp(0, 255).to(torch.uint8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34939/34939 [20:48<00:00, 27.98it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def list_files_recursively(data_dir):\n",
    "    results = []\n",
    "    for entry in sorted(os.listdir(data_dir)):\n",
    "        full_path = os.path.join(data_dir, entry)\n",
    "        ext = os.path.splitext(full_path)[1].lower()\n",
    "        if os.path.isdir(full_path):\n",
    "           results.extend(list_files_recursively(full_path))\n",
    "        elif ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "            results.append(full_path) \n",
    "    return results\n",
    "\n",
    "def center_crop_square(image: torch.Tensor):\n",
    "    # Crop center of image to be square using pytorch\n",
    "    _, width, height = image.shape\n",
    "    new_size = min(width, height)\n",
    "\n",
    "    left = (width - new_size) // 2\n",
    "    top = (height - new_size) // 2\n",
    "    right = (width + new_size) // 2\n",
    "    bottom = (height + new_size) // 2\n",
    "\n",
    "    return image[:, left:right, top:bottom]\n",
    "\n",
    "def resize_square(image: torch.Tensor, size: int): \n",
    "  return torchvision.transforms.functional.resize(image, size, interpolation=torchvision.transforms.InterpolationMode.BICUBIC)\n",
    "\n",
    "image_files = list_files_recursively(IMAGE_FOLDER)\n",
    "\n",
    "# For each image, load it, resize it, and save it   \n",
    "\n",
    "for image_file in tqdm.tqdm(image_files):\n",
    "  # Load image RGB\n",
    "\n",
    "  image = torchvision.io.read_image(image_file, mode=torchvision.io.image.ImageReadMode.RGB)\n",
    "  image = image.to(device=device, dtype=torch.float32)\n",
    "\n",
    "  # Preprocess\n",
    "\n",
    "  image = center_crop_square(image)\n",
    "  image = resize_square(image, TARGET_SIZE)\n",
    "  image = (image / 127.5) - 1\n",
    "  image = image.unsqueeze(0)  \n",
    "\n",
    "  # Encode\n",
    "\n",
    "  with torch.cuda.amp.autocast(): # Imrpoves performance and reduces memory footprint on NVIDIA GPUs\n",
    "    mean, logvar = encode(image)\n",
    "\n",
    "  # Save\n",
    "\n",
    "  mean = mean.cpu().numpy().astype(np.float16)\n",
    "  logvar = logvar.cpu().numpy().astype(np.float16)\n",
    "\n",
    "  npz_file = os.path.splitext(image_file.replace(IMAGE_FOLDER, LATENT_IMAGE_FOLDER))[0] + '.npz'\n",
    "\n",
    "  np.savez_compressed(npz_file, mean=mean, logvar=logvar)\n",
    "\n",
    "  # Load\n",
    "\n",
    "  # loaded = np.load(npz_file)\n",
    "\n",
    "  # mean = loaded['mean']\n",
    "  # logvar = loaded['logvar']\n",
    "\n",
    "  # mean = torch.tensor(mean).to(device).to(torch.float32)\n",
    "  # logvar = torch.tensor(logvar).to(device).to(torch.float32)\n",
    "\n",
    "  # # Decode\n",
    "\n",
    "  # z = sample(mean, logvar)\n",
    "  # x = decode(z).cpu()\n",
    "  # x = x.squeeze(0)\n",
    "\n",
    "  # # Save\n",
    "\n",
    "  # torchvision.io.write_jpeg(x, image_file.replace(IMAGE_FOLDER, LATENT_IMAGE_FOLDER), quality=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
