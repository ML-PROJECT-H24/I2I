{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "import torchvision\n",
    "\n",
    "from diffusers.models import AutoencoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = 'data/DATASET'\n",
    "LATENT_IMAGE_FOLDER = 'data/LATENT_DATASET'\n",
    "TARGET_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def init_vae():\n",
    "    # https://huggingface.co/stabilityai/sd-vae-ft-mse\n",
    "    model: AutoencoderKL = AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-mse').to(device)\n",
    "    # model = torch.compile(model) # TODO we should do this on linux for optimization\n",
    "    model = model.eval()\n",
    "    model.train = False\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "vae = init_vae()\n",
    "scale_factor=0.18215 # scale_factor follows DiT and stable diffusion.\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]: \n",
    "    posterior = vae.encode(x, return_dict=False)[0].parameters\n",
    "    return torch.chunk(posterior, 2, dim=1)    \n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(mean: torch.FloatTensor, logvar: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    z = torch.randn_like(mean)\n",
    "    z = mean + z * std\n",
    "    return z * scale_factor\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode(z) -> torch.Tensor:\n",
    "    x = vae.decode(z / scale_factor, return_dict=False)[0]\n",
    "    x = ((x + 1.0) * 127.5).clamp(0, 255).to(torch.uint8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_files_recursively(data_dir):\n",
    "    results = []\n",
    "    for entry in sorted(os.listdir(data_dir)):\n",
    "        full_path = os.path.join(data_dir, entry)\n",
    "        if(entry.endswith('.jpg')):\n",
    "            results.append(full_path)\n",
    "        elif os.path.isdir(full_path):\n",
    "            results.extend(list_files_recursively(full_path))\n",
    "    return results\n",
    "\n",
    "def center_crop_square(image: torch.Tensor):\n",
    "    # Crop center of image to be square using pytorch\n",
    "    _, width, height = image.shape\n",
    "    new_size = min(width, height)\n",
    "\n",
    "    left = (width - new_size) // 2\n",
    "    top = (height - new_size) // 2\n",
    "    right = (width + new_size) // 2\n",
    "    bottom = (height + new_size) // 2\n",
    "\n",
    "    return image[:, left:right, top:bottom]\n",
    "\n",
    "def resize_square(image: torch.Tensor, size: int): \n",
    "  return torchvision.transforms.functional.resize(image, size, interpolation=torchvision.transforms.InterpolationMode.BICUBIC)\n",
    "\n",
    "image_files = list_files_recursively(IMAGE_FOLDER)\n",
    "\n",
    "# For each image, load it, resize it, and save it   \n",
    "\n",
    "for image_file in image_files:\n",
    "  image = torchvision.io.read_image(image_file).to(device).to(torch.float32)\n",
    "\n",
    "  image = center_crop_square(image)\n",
    "  image = resize_square(image, TARGET_SIZE)\n",
    "  image = (image / 127.5) - 1\n",
    "  image = image.unsqueeze(0)\n",
    "\n",
    "  mean, logvar = encode(image)\n",
    "\n",
    "  mean = mean.cpu().numpy().astype(np.float16)\n",
    "  logvar = logvar.cpu().numpy().astype(np.float16)\n",
    "\n",
    "  np.savez_compressed(image_file.replace(IMAGE_FOLDER, LATENT_IMAGE_FOLDER), mean=mean, logvar=logvar)\n",
    "\n",
    "#   # Load\n",
    "\n",
    "#   loaded = np.load(image_file.replace(IMAGE_FOLDER, LATENT_IMAGE_FOLDER) + '.npz')\n",
    "\n",
    "#   mean = loaded['mean']\n",
    "#   logvar = loaded['logvar']\n",
    "\n",
    "#   mean = torch.tensor(mean).to(device).to(torch.float32)\n",
    "#   logvar = torch.tensor(logvar).to(device).to(torch.float32)\n",
    "\n",
    "#   # Decode\n",
    "\n",
    "#   z = sample(mean, logvar)\n",
    "#   x = decode(z).cpu()\n",
    "#   x = x.squeeze(0)\n",
    "\n",
    "#   # Save\n",
    "\n",
    "#   torchvision.io.write_jpeg(x, image_file.replace(IMAGE_FOLDER, LATENT_IMAGE_FOLDER), quality=100)\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
